from __future__ import annotations

from typing import Literal, Optional

import torch
import torch.nn as nn
from torch import Tensor

from .sde import LatentBridgeSDE


Direction = Literal["forward", "backward"]
InitialCoupling = Literal["paired", "independent"]


class MSBMCouplingSampler:
    """Hybrid coupling sampler for MSBM with tracked trajectories.

    - Stage 1: uses tracked particle pairs (empirical joint coupling).
    - Stage >1: uses policy-sampled coupling (endpoints generated by a frozen policy).

    Notes on direction:
    - direction="forward": train the backward policy (z_b), sample endpoints with z_f.
    - direction="backward": train the forward policy (z_f), sample endpoints with z_b.
    """

    def __init__(
        self,
        latent_trajectories: Tensor,  # (T, N, K)
        t_dists: Tensor,  # (T,)
        sde: LatentBridgeSDE,
        device: str,
        *,
        initial_coupling: InitialCoupling = "paired",
    ):
        if latent_trajectories.ndim != 3:
            raise ValueError(
                f"latent_trajectories must have shape (T, N, K); got {tuple(latent_trajectories.shape)}"
            )
        if t_dists.ndim != 1:
            raise ValueError(f"t_dists must have shape (T,); got {tuple(t_dists.shape)}")
        if latent_trajectories.shape[0] != t_dists.shape[0]:
            raise ValueError("latent_trajectories and t_dists must agree on the time dimension.")

        self.latent_traj = latent_trajectories.to(device)
        self.t_dists = t_dists.to(device)
        self.num_dist = int(t_dists.shape[0])
        self.num_samples = int(latent_trajectories.shape[1])
        self.sde = sde
        self.device = device
        if initial_coupling not in ("paired", "independent"):
            raise ValueError("initial_coupling must be 'paired' or 'independent'.")
        self.initial_coupling: InitialCoupling = initial_coupling

    def sample_coupling(
        self,
        *,
        stage: int,
        direction: Direction,
        policy_impt: Optional[nn.Module],
        batch_size: int,
        ts: Tensor,
        drift_clip_norm: Optional[float] = None,
    ) -> tuple[Tensor, Tensor, Tensor, Tensor]:
        """Generate training pairs (y0, y1, t0, t1) for policy optimization.

        Notes:
            To mirror the reference MSBM code, ``batch_size`` is interpreted as
            the number of samples *per interval* (``samp_bs``). The returned
            tensors therefore have first dimension ``(num_dist - 1) * batch_size``.
        """
        if stage <= 0:
            raise ValueError("stage must be >= 1.")
        if stage == 1:
            if self.initial_coupling == "paired":
                return self._paired_coupling(direction, batch_size)
            return self._independent_coupling(direction, batch_size)
        if policy_impt is None:
            raise ValueError("policy_impt must be provided for stage > 1.")
        return self._policy_coupling(direction, policy_impt, batch_size, ts, drift_clip_norm=drift_clip_norm)

    def _reverse_interval_idx(self, interval_idx: Tensor) -> Tensor:
        """Map forward interval indices to reversed-time interval indices.

        This mirrors `MSBM/runner.py`, which flips `train_t0s/train_t1s` when
        training the backward policy. Concretely, for `num_dist=T` marginals
        (and `T-1` intervals), interval `i` is relabeled to interval
        `(T-2-i)` for backward-policy time conditioning.
        """
        num_intervals = self.num_dist - 1
        return (num_intervals - 1) - interval_idx

    def _paired_coupling(
        self,
        direction: Direction,
        batch_size: int,
    ) -> tuple[Tensor, Tensor, Tensor, Tensor]:
        # Sample `batch_size` points per interval to match MSBM's samp_bs setup.
        num_intervals = self.num_dist - 1
        interval_idx = torch.arange(num_intervals, device=self.device).repeat_interleave(batch_size)
        particle_idx = torch.randint(0, self.num_samples, (interval_idx.shape[0],), device=self.device)

        # Forward-time interval labels (used for the forward policy).
        t0_f = self.t_dists[interval_idx]
        t1_f = self.t_dists[interval_idx + 1]

        # Backward policy is trained on a time-reversed index convention:
        # flip interval labels to mirror MSBM's `train_t0s.flip(...)`.
        rev_idx = self._reverse_interval_idx(interval_idx)
        t0_rev = self.t_dists[rev_idx]
        t1_rev = self.t_dists[rev_idx + 1]

        if direction == "forward":
            # Train backward policy on reversed endpoint order.
            y0 = self.latent_traj[interval_idx + 1, particle_idx]  # later marginal
            y1 = self.latent_traj[interval_idx, particle_idx]      # earlier marginal
            t0, t1 = t0_rev, t1_rev
        else:
            # Train forward policy on forward endpoint order.
            y0 = self.latent_traj[interval_idx, particle_idx]      # earlier marginal
            y1 = self.latent_traj[interval_idx + 1, particle_idx]  # later marginal
            t0, t1 = t0_f, t1_f

        return y0, y1, t0.unsqueeze(-1), t1.unsqueeze(-1)

    def _independent_coupling(
        self,
        direction: Direction,
        batch_size: int,
    ) -> tuple[Tensor, Tensor, Tensor, Tensor]:
        """Stage-1 independent coupling (matches MSBM's default stage-1 behavior)."""
        num_intervals = self.num_dist - 1
        interval_idx = torch.arange(num_intervals, device=self.device).repeat_interleave(batch_size)

        idx0 = torch.randint(0, self.num_samples, (interval_idx.shape[0],), device=self.device)
        idx1 = torch.randint(0, self.num_samples, (interval_idx.shape[0],), device=self.device)

        # Forward-time interval labels (used for the forward policy).
        t0_f = self.t_dists[interval_idx]
        t1_f = self.t_dists[interval_idx + 1]

        # Backward policy uses reversed-time interval labels (MSBM flip).
        rev_idx = self._reverse_interval_idx(interval_idx)
        t0_rev = self.t_dists[rev_idx]
        t1_rev = self.t_dists[rev_idx + 1]

        if direction == "forward":
            # Train backward policy on reversed endpoint order.
            y0 = self.latent_traj[interval_idx + 1, idx0]  # later marginal
            y1 = self.latent_traj[interval_idx, idx1]      # earlier marginal
            t0, t1 = t0_rev, t1_rev
        else:
            # Train forward policy on forward endpoint order.
            y0 = self.latent_traj[interval_idx, idx0]      # earlier marginal
            y1 = self.latent_traj[interval_idx + 1, idx1]  # later marginal
            t0, t1 = t0_f, t1_f

        return y0, y1, t0.unsqueeze(-1), t1.unsqueeze(-1)

    def _policy_coupling(
        self,
        direction: Direction,
        policy_impt: nn.Module,
        batch_size: int,
        ts: Tensor,
        *,
        drift_clip_norm: Optional[float] = None,
    ) -> tuple[Tensor, Tensor, Tensor, Tensor]:
        num_intervals = self.num_dist - 1
        interval_idx = torch.arange(num_intervals, device=self.device).repeat_interleave(batch_size)
        particle_idx = torch.randint(0, self.num_samples, (interval_idx.shape[0],), device=self.device)

        # Forward-time labels for the (i -> i+1) interval.
        t0_f = self.t_dists[interval_idx].unsqueeze(-1)
        t1_f = self.t_dists[interval_idx + 1].unsqueeze(-1)

        # Reversed-time labels used when running the backward policy (mirrors MSBM).
        rev_idx = self._reverse_interval_idx(interval_idx)
        t0_rev = self.t_dists[rev_idx].unsqueeze(-1)
        t1_rev = self.t_dists[rev_idx + 1].unsqueeze(-1)

        if direction == "forward":
            # Train backward policy: sample endpoints by integrating the frozen forward policy.
            y_init = self.latent_traj[interval_idx, particle_idx]  # earlier
            _, y_final = self.sde.sample_traj(
                ts,
                policy_impt,
                y_init,
                t0_f,
                t_final=t1_f,
                save_traj=False,
                drift_clip_norm=drift_clip_norm,
            )

            # Reversed endpoint order for backward policy regression.
            y0 = y_final
            y1 = y_init
            # Backward policy time-conditioning uses reversed-time labels (MSBM flip).
            t0 = t0_rev
            t1 = t1_rev

        else:
            # Train forward policy: sample endpoints by integrating the frozen backward policy.
            y_init = self.latent_traj[interval_idx + 1, particle_idx]  # later
            # Run the backward policy on reversed-time labels to emulate sampling
            # from (t_{i+1} -> t_i) while integrating forward in local time.
            _, y_final = self.sde.sample_traj(
                ts,
                policy_impt,
                y_init,
                t0_rev,
                t_final=t1_rev,
                save_traj=False,
                drift_clip_norm=drift_clip_norm,
            )

            # Forward endpoint order for forward policy regression.
            y0 = y_final
            y1 = y_init
            # Forward policy uses forward-time labels.
            t0 = t0_f
            t1 = t1_f

        return y0, y1, t0, t1
